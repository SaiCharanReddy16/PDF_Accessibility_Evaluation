{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6kMLAWYTCO2",
        "outputId": "0c15cec5-c80a-4f93-f6e7-e26f054f7ecc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.23.5-cp310-none-manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.23.5 (from PyMuPDF)\n",
            "  Downloading PyMuPDFb-1.23.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.23.5 PyMuPDFb-1.23.5\n"
          ]
        }
      ],
      "source": [
        "!pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpEbXvbsUkUm",
        "outputId": "376052d5-aba8-4a97-ecca-8c85ac2f3d1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 19 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.2 [186 kB]\n",
            "Fetched 186 kB in 1s (127 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 120874 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.2) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 19 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 2s (2,006 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 120904 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (9.4.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n",
            "Installing collected packages: pytesseract, pdf2image\n",
            "Successfully installed pdf2image-1.16.3 pytesseract-0.3.10\n"
          ]
        }
      ],
      "source": [
        "# Install Poppler\n",
        "!apt-get install poppler-utils\n",
        "\n",
        "# Install Tesseract\n",
        "!apt-get install tesseract-ocr\n",
        "\n",
        "# Install Python packages\n",
        "!pip install pdf2image pytesseract\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nX0rFaMvQa7O",
        "outputId": "122d5aea-86db-4828-8810-1512b1fb7614"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (1.16.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (9.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pdf2image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MXfSX4TULo9",
        "outputId": "54d18c24-4ae9-4a1d-e964-8ba4d3641c8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytesseract pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NK3oi2iiN3g2",
        "outputId": "6898462b-98e7-4ed0-a30c-bc2465489434"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: PyMuPDFb==1.23.5 in /usr/local/lib/python3.10/dist-packages (from PyMuPDF) (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzaYa8_ST6W5",
        "outputId": "3c3f45d6-c3a4-431d-b8d0-049ecc7fe3e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting imagehash\n",
            "  Downloading ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/296.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/296.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.5/296.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyWavelets in /usr/local/lib/python3.10/dist-packages (from imagehash) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imagehash) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from imagehash) (9.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from imagehash) (1.11.3)\n",
            "Installing collected packages: imagehash\n",
            "Successfully installed imagehash-4.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install imagehash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QB40xwVcUZfE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q02RfgfoaHGU",
        "outputId": "7111b569-f475-464c-bf1e-2517662a99d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imagehash in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.10/dist-packages (from imagehash) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imagehash) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from imagehash) (9.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from imagehash) (1.11.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install imagehash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2kG0Hzbg1fW",
        "outputId": "6a922f3e-71fd-4ce8-938a-d8565a851bc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.10.3-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.0/49.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20221105 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.22.0-py3-none-manylinux_2_17_x86_64.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (3.3.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (41.0.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (2.21)\n",
            "Installing collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20221105 pdfplumber-0.10.3 pypdfium2-4.22.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h79r5_kLh3qi",
        "outputId": "f9a2087c-ac14-4199-ce67-71016ad55783"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.10.3)\n",
            "Requirement already satisfied: pdfminer.six==20221105 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20221105)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.22.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (3.3.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (41.0.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (2.21)\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKACU2ePYxED",
        "outputId": "219278f8-893f-4564-9657-475475d49752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL: https://www.linkedin.com/in/vasudhamalla/\n",
            "Page: 1\n",
            "\n",
            "URL: mailto:vasudha.malla@gmail.com%7C\n",
            "Page: 1\n",
            "\n",
            "URL: https://github.com/vasudhamalla\n",
            "Page: 1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import fitz\n",
        "\n",
        "def extract_links(pdf_file_path):\n",
        "    links = []\n",
        "    unique_urls = set()\n",
        "    pdf_document = fitz.open(pdf_file_path)\n",
        "\n",
        "    for page_num in range(len(pdf_document)):\n",
        "        page = pdf_document.load_page(page_num)\n",
        "        page_links = page.get_links()\n",
        "        page_text = page.get_text(\"dict\")\n",
        "\n",
        "        for link in page_links:\n",
        "            link_url = link.get(\"uri\")\n",
        "            text_value = None\n",
        "\n",
        "            for block in page_text.get('blocks', []):\n",
        "                for line in block.get('lines', []):\n",
        "                    for span in line.get('spans', []):\n",
        "                        if 'text' in span:\n",
        "                            text_value = span['text']\n",
        "                            break\n",
        "            if link_url not in unique_urls:\n",
        "                links.append({\n",
        "                    \"text\": text_value,\n",
        "                    \"url\": link_url,\n",
        "                    \"page\": page_num + 1\n",
        "                })\n",
        "                unique_urls.add(link_url)\n",
        "\n",
        "    pdf_document.close()\n",
        "    return links\n",
        "\n",
        "input_pdf_file = \"malla_vasudha_resume.pdf\"\n",
        "\n",
        "extracted_links = extract_links(input_pdf_file)\n",
        "\n",
        "for link_info in extracted_links:\n",
        "    print(f\"URL: {link_info['url']}\")\n",
        "    print(f\"Page: {link_info['page']}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgxehUNXhSOO",
        "outputId": "fb24dab4-a25c-4f34-c571-e2fac403626d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL: https://unt.instructure.com/login/ldap https://joinhandshake.com/\n",
            "Page: 1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pdfplumber\n",
        "from urllib.parse import urlparse\n",
        "def is_valid_url(string):\n",
        "    parsed_url = urlparse(string)\n",
        "    return all([parsed_url.scheme, parsed_url.netloc])\n",
        "\n",
        "def extract_links(pdf_file_path):\n",
        "    links = []\n",
        "\n",
        "    with pdfplumber.open(pdf_file_path) as pdf:\n",
        "        for page_num, page in enumerate(pdf.pages):\n",
        "            text = page.extract_text()\n",
        "\n",
        "            words = text.split()\n",
        "            url_parts = []\n",
        "            in_url = False\n",
        "\n",
        "            for word in words:\n",
        "                if is_valid_url(word):\n",
        "                    if in_url:\n",
        "                        url_parts.append(word)\n",
        "                    else:\n",
        "                        in_url = True\n",
        "                        url_parts = [word]\n",
        "                else:\n",
        "                    if in_url:\n",
        "                        in_url = False\n",
        "                        url = \" \".join(url_parts)\n",
        "                        links.append({\n",
        "                            \"url\": url,\n",
        "                            \"page\": page_num + 1\n",
        "                        })\n",
        "            if in_url:\n",
        "                url = \" \".join(url_parts)\n",
        "                links.append({\n",
        "                    \"url\": url,\n",
        "                    \"page\": page_num + 1\n",
        "                })\n",
        "\n",
        "    return links\n",
        "\n",
        "input_pdf_file = \"e1.pdf\"\n",
        "\n",
        "extracted_links = extract_links(input_pdf_file)\n",
        "\n",
        "for link_info in extracted_links:\n",
        "    print(f\"URL: {link_info['url']}\")\n",
        "    print(f\"Page: {link_info['page']}\")\n",
        "    print()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olm0HWMulnuj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "#HyperLink Extraction bug fixing\n",
        "import fitz\n",
        "\n",
        "def extract_links(pdf_file_path):\n",
        "    links = []\n",
        "    unique_urls = set()\n",
        "    pdf_document = fitz.open(pdf_file_path)\n",
        "\n",
        "    for page_num in range(len(pdf_document)):\n",
        "        page = pdf_document.load_page(page_num)\n",
        "        page_links = page.get_links()\n",
        "        page_text = page.get_text(\"dict\")\n",
        "\n",
        "        for link in page_links:\n",
        "            link_url = link.get(\"uri\")\n",
        "            text_value = None\n",
        "\n",
        "            for block in page_text.get('blocks', []):\n",
        "              for line in block.get('lines', []):\n",
        "                for span in line.get('spans', []):\n",
        "                  if 'text' in span :\n",
        "                    text_value = span['text']\n",
        "                    break  # Exit the loop if a matching link text is found\n",
        "                if text_value:  # Exit the loop if a matching link text is found\n",
        "                  break\n",
        "              if text_value:\n",
        "                break\n",
        "\n",
        "\n",
        "            if link_url not in unique_urls:\n",
        "                links.append({\n",
        "                    \"text\": text_value,\n",
        "                    \"url\": link_url,\n",
        "                    \"page\": page_num + 1\n",
        "                })\n",
        "                unique_urls.add(link_url)\n",
        "\n",
        "\n",
        "    pdf_document.close()\n",
        "    return links\n",
        "\n",
        "input_pdf_file = \"malla_vasudha_resume.pdf\"\n",
        "\n",
        "extracted_links = extract_links(input_pdf_file)\n",
        "\n",
        "for link_info in extracted_links:\n",
        "    print(f\"URL: {link_info['url']}\")\n",
        "    print(f\"Page: {link_info['page']}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eM71sH3u2dqf",
        "outputId": "5713fb41-419a-42fb-d1ab-5d19f7a0a9a8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL: https://www.linkedin.com/in/vasudhamalla/\n",
            "Page: 1\n",
            "\n",
            "URL: mailto:vasudha.malla@gmail.com%7C\n",
            "Page: 1\n",
            "\n",
            "URL: https://github.com/vasudhamalla\n",
            "Page: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperlink Extraction after refactoring with Extract method\n",
        "import fitz\n",
        "\n",
        "def extract_text_from_page(page):\n",
        "    page_text = page.get_text(\"dict\")\n",
        "    for block in page_text.get('blocks', []):\n",
        "        for line in block.get('lines', []):\n",
        "            for span in line.get('spans', []):\n",
        "                if 'text' in span:\n",
        "                    return span['text']\n",
        "    return None\n",
        "\n",
        "def extract_links(pdf_file_path):\n",
        "    links = []\n",
        "    unique_urls = set()\n",
        "    pdf_document = fitz.open(pdf_file_path)\n",
        "\n",
        "    for page_num in range(len(pdf_document)):\n",
        "        page = pdf_document.load_page(page_num)\n",
        "        page_links = page.get_links()\n",
        "        text_value = extract_text_from_page(page)  # Extract text using the new method\n",
        "\n",
        "        for link in page_links:\n",
        "            link_url = link.get(\"uri\")\n",
        "\n",
        "            if link_url not in unique_urls:\n",
        "                links.append({\n",
        "                    \"text\": text_value,\n",
        "                    \"url\": link_url,\n",
        "                    \"page\": page_num + 1\n",
        "                })\n",
        "                unique_urls.add(link_url)\n",
        "\n",
        "    pdf_document.close()\n",
        "    return links\n",
        "\n",
        "input_pdf_file = \"malla_vasudha_resume.pdf\"\n",
        "\n",
        "extracted_links = extract_links(input_pdf_file)\n",
        "\n",
        "for link_info in extracted_links:\n",
        "    print(f\"URL: {link_info['url']}\")\n",
        "    print(f\"Page: {link_info['page']}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4v5VaXt8ByVn",
        "outputId": "cd484b39-898c-48dd-8d5e-b34ea8821e84"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL: https://www.linkedin.com/in/vasudhamalla/\n",
            "Page: 1\n",
            "\n",
            "URL: mailto:vasudha.malla@gmail.com%7C\n",
            "Page: 1\n",
            "\n",
            "URL: https://github.com/vasudhamalla\n",
            "Page: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install validators\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZq9CV4X9yX8",
        "outputId": "b4caf2e3-567b-453a-f873-d805b5ecfd3e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting validators\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: validators\n",
            "Successfully installed validators-0.22.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Link Extraction after bug fixing\n",
        "import pdfplumber\n",
        "import re\n",
        "from urllib.parse import urlsplit\n",
        "import validators\n",
        "\n",
        "def extract_links(pdf_file_path):\n",
        "    links = []\n",
        "\n",
        "    with pdfplumber.open(pdf_file_path) as pdf:\n",
        "        for page_num, page in enumerate(pdf.pages):\n",
        "            text = page.extract_text()\n",
        "\n",
        "            # Improved URL detection using regular expressions\n",
        "            url_pattern = re.compile(r'https?://\\S+(?:#\\S+)?')\n",
        "            urls = re.findall(url_pattern, text)\n",
        "\n",
        "            for url in urls:\n",
        "                # Validate the URL using the validators library\n",
        "                if validators.url(url):\n",
        "                    parsed_url = urlsplit(url)\n",
        "                    links.append({\n",
        "                        \"url\": url,\n",
        "                        \"page\": page_num + 1\n",
        "                    })\n",
        "\n",
        "    return links\n",
        "\n",
        "input_pdf_file = \"e1.pdf\"\n",
        "extracted_links = extract_links(input_pdf_file)\n",
        "\n",
        "for link_info in extracted_links:\n",
        "    print(f\"URL: {link_info['url']}\")\n",
        "    print(f\"Page: {link_info['page']}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5k89yy57Yrv",
        "outputId": "abde9c83-5fbe-4639-d105-1f8319655067"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL: https://unt.instructure.com/login/ldap\n",
            "Page: 1\n",
            "\n",
            "URL: https://joinhandshake.com/\n",
            "Page: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Link Extraction After refactoring\n",
        "\n",
        "import pdfplumber\n",
        "import re\n",
        "from urllib.parse import urlsplit\n",
        "import validators\n",
        "\n",
        "def extract_links_from_text(text, page_num):\n",
        "    url_pattern = re.compile(r'https?://\\S+(?:#\\S+)?')\n",
        "    urls = re.findall(url_pattern, text)\n",
        "\n",
        "    valid_links = []\n",
        "    for url in urls:\n",
        "        if validators.url(url):\n",
        "            parsed_url = urlsplit(url)\n",
        "            valid_links.append({\n",
        "                \"url\": url,\n",
        "                \"page\": page_num\n",
        "            })\n",
        "    return valid_links\n",
        "\n",
        "def extract_links(pdf_file_path):\n",
        "    links = []\n",
        "\n",
        "    with pdfplumber.open(pdf_file_path) as pdf:\n",
        "        for page_num, page in enumerate(pdf.pages):\n",
        "            text = page.extract_text()\n",
        "            page_links = extract_links_from_text(text, page_num + 1)\n",
        "            links.extend(page_links)\n",
        "\n",
        "    return links\n",
        "\n",
        "input_pdf_file = \"e1.pdf\"\n",
        "extracted_links = extract_links(input_pdf_file)\n",
        "\n",
        "for link_info in extracted_links:\n",
        "    print(f\"URL: {link_info['url']}\")\n",
        "    print(f\"Page: {link_info['page']}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKQTWKTTGiPU",
        "outputId": "37bfca02-e67a-4a03-f549-9773067a6b76"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL: https://unt.instructure.com/login/ldap\n",
            "Page: 1\n",
            "\n",
            "URL: https://joinhandshake.com/\n",
            "Page: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperlink Extraction after refactoring with Encapsulated function\n",
        "import fitz\n",
        "\n",
        "class PDFLinkExtractor:\n",
        "    def __init__(self):\n",
        "        self.links = []\n",
        "        self.unique_urls = set()\n",
        "\n",
        "    def extract_text_from_page(self, page):\n",
        "        page_text = page.get_text(\"dict\")\n",
        "        for block in page_text.get('blocks', []):\n",
        "            for line in block.get('lines', []):\n",
        "                for span in line.get('spans', []):\n",
        "                    if 'text' in span:\n",
        "                        return span['text']\n",
        "        return None\n",
        "\n",
        "    def extract_links(self, pdf_file_path):\n",
        "        pdf_document = fitz.open(pdf_file_path)\n",
        "\n",
        "        for page_num in range(len(pdf_document)):\n",
        "            page = pdf_document.load_page(page_num)\n",
        "            page_links = page.get_links()\n",
        "            text_value = self.extract_text_from_page(page)\n",
        "\n",
        "            for link in page_links:\n",
        "                link_url = link.get(\"uri\")\n",
        "\n",
        "                if link_url not in self.unique_urls:\n",
        "                    self.links.append({\n",
        "                        \"text\": text_value,\n",
        "                        \"url\": link_url,\n",
        "                        \"page\": page_num + 1\n",
        "                    })\n",
        "                    self.unique_urls.add(link_url)\n",
        "\n",
        "        pdf_document.close()\n",
        "        return self.links\n",
        "\n",
        "input_pdf_file = \"malla_vasudha_resume.pdf\"\n",
        "\n",
        "pdf_link_extractor = PDFLinkExtractor()\n",
        "extracted_links = pdf_link_extractor.extract_links(input_pdf_file)\n",
        "\n",
        "for link_info in extracted_links:\n",
        "    print(f\"URL: {link_info['url']}\")\n",
        "    print(f\"Page: {link_info['page']}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZKRoRzvXtp0",
        "outputId": "43ea0156-a516-4306-cb28-44786d0201f6"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL: https://www.linkedin.com/in/vasudhamalla/\n",
            "Page: 1\n",
            "\n",
            "URL: mailto:vasudha.malla@gmail.com%7C\n",
            "Page: 1\n",
            "\n",
            "URL: https://github.com/vasudhamalla\n",
            "Page: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperlink Extraction after refactoring with Encapsulated methods\n",
        "import fitz\n",
        "\n",
        "class PDFLinkExtractor:\n",
        "    def __init__(self):\n",
        "        self._unique_urls = set()\n",
        "\n",
        "    def _get_unique_urls(self):\n",
        "        return self._unique_urls.copy()\n",
        "\n",
        "    def _add_unique_url(self, url):\n",
        "        self._unique_urls.add(url)\n",
        "\n",
        "    def extract_links(self, pdf_file_path):\n",
        "        links = []\n",
        "        pdf_document = fitz.open(pdf_file_path)\n",
        "\n",
        "        for page_num in range(len(pdf_document)):\n",
        "            page = pdf_document.load_page(page_num)\n",
        "            page_links = page.get_links()\n",
        "            text_value = self._extract_text_from_page(page)\n",
        "\n",
        "            for link in page_links:\n",
        "                link_url = link.get(\"uri\")\n",
        "\n",
        "                if link_url not in self._get_unique_urls():\n",
        "                    links.append({\n",
        "                        \"text\": text_value,\n",
        "                        \"url\": link_url,\n",
        "                        \"page\": page_num + 1\n",
        "                    })\n",
        "                    self._add_unique_url(link_url)\n",
        "\n",
        "        pdf_document.close()\n",
        "        return links\n",
        "\n",
        "    def _extract_text_from_page(self, page):\n",
        "        # Extract text logic remains the same as in the previously refactored code\n",
        "        pass\n",
        "\n",
        "input_pdf_file = \"malla_vasudha_resume.pdf\"\n",
        "pdf_link_extractor = PDFLinkExtractor()\n",
        "extracted_links = pdf_link_extractor.extract_links(input_pdf_file)\n",
        "\n",
        "for link_info in extracted_links:\n",
        "    print(f\"URL: {link_info['url']}\")\n",
        "    print(f\"Page: {link_info['page']}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNvwiffCafdQ",
        "outputId": "d79bcbb9-4dbd-453e-d774-19f5c376166a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL: https://www.linkedin.com/in/vasudhamalla/\n",
            "Page: 1\n",
            "\n",
            "URL: mailto:vasudha.malla@gmail.com%7C\n",
            "Page: 1\n",
            "\n",
            "URL: https://github.com/vasudhamalla\n",
            "Page: 1\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}